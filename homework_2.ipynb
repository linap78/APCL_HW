{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20f786e",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор (любой) с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129c4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4314de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "875cc447-3eac-4029-9e4e-70dd9fa4c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True, random_state = 42, stratify = data.toxic.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2477b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from razdel import tokenize\n",
    "\n",
    "def razdel_tokenizer(text):\n",
    "    return [token.text for token in tokenize(text)]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "count_vectorizer_razdel = CountVectorizer(tokenizer=razdel_tokenizer)\n",
    "tfidf_vectorizer_razdel = TfidfVectorizer(tokenizer=razdel_tokenizer)\n",
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b00891a-7c21-43c3-9ac8-3c920bc376e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizing(vectorizer, train, test):\n",
    "\n",
    "    X = vectorizer.fit_transform(train.comment).toarray()\n",
    "    X_test = vectorizer.transform(test.comment).toarray()\n",
    "    y = train.toxic.values\n",
    "    y_test = test.toxic.values\n",
    "\n",
    "    return X, X_test, y, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "387abdb1-9322-407d-8a64-992ab215b591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, X_test, y, y_test, classifier, proba=False):\n",
    "\n",
    "    clf = classifier\n",
    "    clf.fit(X, y)\n",
    "    preds = clf.predict(X_test)\n",
    "    print(classification_report(y_test, preds, zero_division=0))\n",
    "    if proba:\n",
    "        pred_proba = clf.predict_proba(X_test)\n",
    "        return clf, preds, pred_proba\n",
    "    return clf, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8e3448-e952-4d58-8792-033e944a666e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StepanovaAM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_count, X_test_count, y_count, y_test_count = vectorizing(count_vectorizer, train, test)\n",
    "X_count_razdel, X_test_count_razdel, y_count_razdel, y_test_count_razdel = vectorizing(count_vectorizer_razdel, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "367c1774-391a-4736-9ac6-2ccc34f33369",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf, X_test_tfidf, y_tfidf, y_test_tfidf = vectorizing(tfidf_vectorizer, train, test)\n",
    "X_tfidf_razdel, X_test_tfidf_razdel, y_tfidf_razdel, y_test_tfidf_razdel = vectorizing(tfidf_vectorizer_razdel, train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5679bc84-71e9-4f25-b73c-430198c5ee9e",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f519328d-89d3-4791-8ca6-14df5577950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.94      0.90       959\n",
      "         1.0       0.86      0.70      0.77       483\n",
      "\n",
      "    accuracy                           0.86      1442\n",
      "   macro avg       0.86      0.82      0.84      1442\n",
      "weighted avg       0.86      0.86      0.86      1442\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), array([1., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(X_count, X_test_count, y_count, y_test_count, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47b7c0-8012-4af1-bce2-ba79f41d8f4d",
   "metadata": {},
   "source": [
    "### razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78f7c1db-6b05-4081-96f9-eef2002ef25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StepanovaAM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.94      0.90       959\n",
      "         1.0       0.86      0.72      0.78       483\n",
      "\n",
      "    accuracy                           0.87      1442\n",
      "   macro avg       0.86      0.83      0.84      1442\n",
      "weighted avg       0.87      0.87      0.86      1442\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), array([1., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(X_count_razdel, X_test_count_razdel, y_count_razdel, y_test_count_razdel, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b134308f-1fca-4832-bb0b-a8693c2cca6b",
   "metadata": {},
   "source": [
    "## TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf44e9a4-28d3-4dd9-b3e9-0aff160a6820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.96      0.89       959\n",
      "         1.0       0.88      0.60      0.71       483\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.85      0.78      0.80      1442\n",
      "weighted avg       0.85      0.84      0.83      1442\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), array([1., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(X_tfidf, X_test_tfidf, y_tfidf, y_test_tfidf, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c79d35-754f-4e76-a597-d4cae50d9907",
   "metadata": {},
   "source": [
    "### razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2fab095-feee-475f-8969-a9a96add3f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.96      0.89       959\n",
      "         1.0       0.88      0.60      0.72       483\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.85      0.78      0.80      1442\n",
      "weighted avg       0.85      0.84      0.83      1442\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(), array([1., 0., 0., ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(X_tfidf_razdel, X_test_tfidf_razdel, y_tfidf_razdel, y_test_tfidf_razdel, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c5811-f025-4214-af43-6700d64afe2b",
   "metadata": {},
   "source": [
    "результат здесь повышается при токенизации не сильно, но кажется, это неудачный сид, раньше результаты значительно улучшались"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9076e",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e25357",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de962ad",
   "metadata": {},
   "source": [
    "Требования к моделям:   \n",
    "а) один классификатор должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров (можно ставить разные параметры tfidfvectorizer и countvectorizer)  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра (по возможности)  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  \n",
    "\n",
    "*random_seed не считается за параметр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2540a226-9399-4212-b9d3-cd5a1de56a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\StepanovaAM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7ff93fb-b239-496b-8b6a-45cc50e19492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy3\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "  tokens = tokenize(text)\n",
    "  lemmas = []\n",
    "  for token in tokens:\n",
    "    parsed = morph.parse(token.text)[0]\n",
    "    if \"PNCT\" not in parsed.tag:\n",
    "      lemmas.append(parsed.normal_form)\n",
    "  return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7577aabb-c0d2-4b42-aa9b-83e144286990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StepanovaAM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['большой', 'весь', 'всё', 'ещё', 'мочь', 'нибыть', 'свой', 'хороший', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words=russian_stopwords, preprocessor=lemmatize_text, ngram_range=(1, 2), min_df=0.0001, max_df=0.99)\n",
    "X_count, X_test_count, y_count, y_test_count = vectorizing(count_vectorizer, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ac0a5471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.94      0.91       959\n",
      "         1.0       0.86      0.73      0.79       483\n",
      "\n",
      "    accuracy                           0.87      1442\n",
      "   macro avg       0.87      0.83      0.85      1442\n",
      "weighted avg       0.87      0.87      0.87      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg_clf = LogisticRegression(class_weight=\"balanced\", max_iter=110, random_state=42)\n",
    "logreg_clf, preds, proba = training(X_count, X_test_count, y_count, y_test_count, clf, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9326d519-0161-43a1-ab59-93e11b9f0992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\StepanovaAM\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['большой', 'весь', 'всё', 'ещё', 'мочь', 'нибыть', 'свой', 'хороший', 'это'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words=russian_stopwords, preprocessor=lemmatize_text, ngram_range=(1, 2), min_df=0.0001, max_df=0.99)\n",
    "X_tfidf, X_test_tfidf, y_tfidf, y_test_tfidf = vectorizing(tfidf_vectorizer, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b5dddf01-52a9-438f-bbad-6ae60ac55278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.91      0.88       959\n",
      "         1.0       0.80      0.70      0.75       483\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.83      0.80      0.81      1442\n",
      "weighted avg       0.84      0.84      0.84      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=50, random_state=42, class_weight=\"balanced\")\n",
    "forest_clf, preds_forest, proba_forest = training(X_count, X_test_count, y_count, y_test_count, forest_clf, proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232eec5b-4311-408d-9fbf-c299175c0742",
   "metadata": {},
   "source": [
    "Самые токсичные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "31acab19-e7aa-455d-938a-eeb0a4b2c768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>proba_log</th>\n",
       "      <th>proba_tree</th>\n",
       "      <th>proba_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3016</th>\n",
       "      <td>блеаадить как же обидно когда создавать тред п...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6554</th>\n",
       "      <td>по мексиканск флаг ублюдок мать твой а ну идти...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14224</th>\n",
       "      <td>01 30 о встреча с чубайс 02 40 как шария защищ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13982</th>\n",
       "      <td>что какой денежный петух нет у хохлов свой ска...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.853235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>перепутать в роддом негритёнок хохол и русский...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.999981e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12849</th>\n",
       "      <td>есть производитель и ритейл и у каждый свой ос...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.149573e-13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13693</th>\n",
       "      <td>росстат сообщить о рекордный за почти два год ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.935072e-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8813</th>\n",
       "      <td>потому что индустрия игра не благотворительнос...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.244532e-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13973</th>\n",
       "      <td>населённый пункт сша вашингтон город в округ к...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.584313e-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>ну формально причина отказ они не обязать сооб...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.427116e-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1442 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic     proba_log  \\\n",
       "3016   блеаадить как же обидно когда создавать тред п...    1.0  1.000000e+00   \n",
       "6554   по мексиканск флаг ублюдок мать твой а ну идти...    1.0  1.000000e+00   \n",
       "14224  01 30 о встреча с чубайс 02 40 как шария защищ...    1.0  1.000000e+00   \n",
       "13982  что какой денежный петух нет у хохлов свой ска...    1.0  1.000000e+00   \n",
       "551    перепутать в роддом негритёнок хохол и русский...    1.0  9.999981e-01   \n",
       "...                                                  ...    ...           ...   \n",
       "12849  есть производитель и ритейл и у каждый свой ос...    0.0  2.149573e-13   \n",
       "13693  росстат сообщить о рекордный за почти два год ...    0.0  6.935072e-16   \n",
       "8813   потому что индустрия игра не благотворительнос...    0.0  1.244532e-19   \n",
       "13973  населённый пункт сша вашингтон город в округ к...    0.0  5.584313e-24   \n",
       "10875  ну формально причина отказ они не обязать сооб...    0.0  4.427116e-24   \n",
       "\n",
       "       proba_tree  proba_forest  \n",
       "3016          0.0      0.620000  \n",
       "6554          1.0      0.760000  \n",
       "14224         1.0      0.220000  \n",
       "13982         1.0      0.853235  \n",
       "551           1.0      0.900000  \n",
       "...           ...           ...  \n",
       "12849         1.0      0.060000  \n",
       "13693         0.0      0.100000  \n",
       "8813          0.0      0.100000  \n",
       "13973         0.0      0.100000  \n",
       "10875         0.0      0.140000  \n",
       "\n",
       "[1442 rows x 5 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"proba_log\"] = proba[:,1]\n",
    "test[\"proba_forest\"] = proba_forest[:,1]\n",
    "test.sort_values(by=[\"proba_log\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c6a2a97c-97a4-47ae-b355-56db824cd707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "      <th>proba_log</th>\n",
       "      <th>proba_tree</th>\n",
       "      <th>proba_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13589</th>\n",
       "      <td>у они от веганство совсем мозг высохнуть</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.821210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13792</th>\n",
       "      <td>у он хохол быть в предок</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.897903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12343</th>\n",
       "      <td>не русский армянский национальность а россиянка</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.809592</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>vladtime сука что у ты в голова</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.941156</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>кто угодный но не этот два отброс свинья и нед...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.960799</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9548</th>\n",
       "      <td>за всё коммунальный услуга кроме оприборенный ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8480</th>\n",
       "      <td>я про целесообразность покупка новый лад веста...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9111</th>\n",
       "      <td>и всё таки я когда-нибудь посидеть и срисовать...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>вы описать как надо эксплуатировать ssd для до...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.079451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10263</th>\n",
       "      <td>в магазин скидка отличный повод прикупить</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1442 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic  proba_log  \\\n",
       "13589           у они от веганство совсем мозг высохнуть    1.0   0.821210   \n",
       "13792                           у он хохол быть в предок    1.0   0.897903   \n",
       "12343    не русский армянский национальность а россиянка    1.0   0.809592   \n",
       "391                      vladtime сука что у ты в голова    1.0   0.941156   \n",
       "5982   кто угодный но не этот два отброс свинья и нед...    1.0   0.960799   \n",
       "...                                                  ...    ...        ...   \n",
       "9548   за всё коммунальный услуга кроме оприборенный ...    0.0   0.000061   \n",
       "8480   я про целесообразность покупка новый лад веста...    0.0   0.000159   \n",
       "9111   и всё таки я когда-нибудь посидеть и срисовать...    0.0   0.072918   \n",
       "8520   вы описать как надо эксплуатировать ssd для до...    0.0   0.079451   \n",
       "10263          в магазин скидка отличный повод прикупить    0.0   0.055418   \n",
       "\n",
       "       proba_tree  proba_forest  \n",
       "13589         1.0           1.0  \n",
       "13792         1.0           1.0  \n",
       "12343         1.0           1.0  \n",
       "391           1.0           1.0  \n",
       "5982          1.0           1.0  \n",
       "...           ...           ...  \n",
       "9548          0.0           0.0  \n",
       "8480          0.0           0.0  \n",
       "9111          0.0           0.0  \n",
       "8520          0.0           0.0  \n",
       "10263         0.0           0.0  \n",
       "\n",
       "[1442 rows x 5 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.sort_values(by=[\"proba_forest\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf4f7c-362b-467d-b45e-c9c806c8d9ba",
   "metadata": {},
   "source": [
    "- везде в топ-5 вошли токсичные комменты\n",
    "- однако они не совпадают у разных классификаторов\n",
    "- но веростность того, что что-то из топа-5 по логрегу принадлежит токсичному классу, согласно лесу, низка, логрег же считает комменты из топа-5 леса довольно токсичными\n",
    "- более того, комменты, отмеченные токсичными логрегом, кажутся мне более токсичными"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f228c3e",
   "metadata": {},
   "source": [
    "## Задание 3 (4 балла - 1 балл за каждый классификатор)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566929b7",
   "metadata": {},
   "source": [
    "Для классификаторов Logistic Regression, Decision Trees, Naive Bayes, RandomForest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию. \n",
    "Также как и в предыдущем задании у классификаторов должно быть задано вручную как минимум 2 параметра (по возможности, f1 мера каждого из классификаторов должна быть минимум 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d3086-ff23-4217-bbaf-3aafd7a5b5f5",
   "metadata": {},
   "source": [
    "## logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "3dc36e0d-360c-4324-b281-ee9b05c254b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "15af4e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(3.3001844755282184), 'хохол'),\n",
       " (np.float64(3.128510956782556), 'хохлов'),\n",
       " (np.float64(3.01980484354331), 'дебил'),\n",
       " (np.float64(2.61924283319946), 'шлюха'),\n",
       " (np.float64(2.3412840089571794), 'быдло')]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_lr = np.abs(logreg_clf.coef_[0])\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "top_5_lr = sorted(zip(feature_importances_lr, feature_names), reverse=True)[:5]\n",
    "top_5_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d802155-c355-42d8-80af-742c6028f080",
   "metadata": {},
   "source": [
    "## tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "ae12ad75-5175-4b03-9ee0-64e6d84851c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.80      0.82       959\n",
      "         1.0       0.63      0.69      0.66       483\n",
      "\n",
      "    accuracy                           0.76      1442\n",
      "   macro avg       0.73      0.74      0.74      1442\n",
      "weighted avg       0.77      0.76      0.76      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(criterion='log_loss', random_state=42, class_weight=\"balanced\")\n",
    "tree_clf, preds_tree, proba_tree = training(X_tfidf, X_test_tfidf, y_tfidf, y_test_tfidf, tree_clf, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6afd235d-2b45-4e1d-873b-96240d3f93f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(0.024551252126889196), 'это'),\n",
       " (np.float64(0.01912567738796729), 'хохол'),\n",
       " (np.float64(0.016420766775276558), 'год'),\n",
       " (np.float64(0.014971973011969552), 'всё'),\n",
       " (np.float64(0.0136703148926069), 'свой')]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances_dt = tree_clf.feature_importances_\n",
    "\n",
    "top_5_dt = sorted(zip(feature_importances_dt, feature_names), reverse=True)[:5]\n",
    "top_5_dt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed79d4c-d927-46c6-b217-9da4fb32bc1e",
   "metadata": {},
   "source": [
    "## RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a4a63d4-9230-4b36-9d94-c0ae434e1df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.90      0.87       959\n",
      "         1.0       0.78      0.69      0.73       483\n",
      "\n",
      "    accuracy                           0.83      1442\n",
      "   macro avg       0.81      0.79      0.80      1442\n",
      "weighted avg       0.83      0.83      0.83      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(n_estimators=50, random_state=42, class_weight=\"balanced\")\n",
    "forest_clf, preds_forest, proba_forest = training(X_tfidf, X_test_tfidf, y_tfidf, y_test_tfidf, forest_clf, proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79645b9b-4a77-46c0-bccb-5daed5b80d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_rf = forest_clf.feature_importances_\n",
    "top_5_rf = sorted(zip(feature_importances_rf, feature_names), reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57530b29-43de-4d18-bfa5-325f24a0f816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(0.009603642415217472), 'мигрантов'),\n",
       " (np.float64(0.008292873661427988), 'библиотек'),\n",
       " (np.float64(0.006704594624919413), 'мусорного'),\n",
       " (np.float64(0.0065636302393929915), 'атморы'),\n",
       " (np.float64(0.0064755951304561465), 'жидовок')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_5_rf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
