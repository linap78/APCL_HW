{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel -q"
      ],
      "metadata": {
        "id": "2z0n7CeQJwxa"
      },
      "id": "2z0n7CeQJwxa",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "00fad453",
      "metadata": {
        "id": "00fad453"
      },
      "source": [
        "# Домашнее задание № 4. Языковые модели"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1AUanjhJE3V",
        "outputId": "ef0add9c-0898-49fe-b3a2-f63ce9d97e22"
      },
      "id": "r1AUanjhJE3V",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d056af4",
      "metadata": {
        "id": "5d056af4"
      },
      "source": [
        "## Задание 1 (8 баллов)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f532a8",
      "metadata": {
        "id": "d1f532a8"
      },
      "source": [
        "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de743d1d",
      "metadata": {
        "id": "de743d1d"
      },
      "source": [
        "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
        "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели.\n",
        "Можно использовать данные из семинара или любые другие (можно брать только часть текста, если считается слишком долго). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
        "\n",
        "\n",
        "Подсказки:  \n",
        "    - нужно будет добавить еще один тэг \\<start>  \n",
        "    - можете использовать тот же подход с матрицей вероятностей, но по строкам хронить биграмы, а по колонкам униграммы\n",
        "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)\n",
        "    - у вас будут словари с индексами биграммов и униграммов, не перепутайте их при переводе индекса в слово - словарь биграммов будет больше словаря униграммов и все индексы из униграммного словаря будут формально подходить для словаря биграммов (не будет ошибки при id2bigram[unigram_id]), но маппинг при этом будет совершенно неправильным"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение файла"
      ],
      "metadata": {
        "id": "Vvd7xypLVt1X"
      },
      "id": "Vvd7xypLVt1X"
    },
    {
      "cell_type": "code",
      "source": [
        "news = open(\"lenta.txt\").read()"
      ],
      "metadata": {
        "id": "VmEQZ9oHJo95"
      },
      "id": "VmEQZ9oHJo95",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нормализация:"
      ],
      "metadata": {
        "id": "2IGjlnENVx9y"
      },
      "id": "2IGjlnENVx9y"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d078056d",
      "metadata": {
        "id": "d078056d"
      },
      "outputs": [],
      "source": [
        "def normalize(text):\n",
        "    normalized_text = [word.text.strip(punctuation) for word \\\n",
        "                                                            in razdel_tokenize(text)]\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
        "    return normalized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XXgvLwUhV2KD"
      },
      "id": "XXgvLwUhV2KD"
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrammer(tokens, n):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "UxaZIwngMKxG"
      },
      "id": "UxaZIwngMKxG",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6afcef88",
      "metadata": {
        "id": "6afcef88"
      },
      "outputs": [],
      "source": [
        "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news[:500000])]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents_news_train, sents_news_val = train_test_split(sentences_news, test_size=50, random_state=42)"
      ],
      "metadata": {
        "id": "UzMoTFKKLjOP"
      },
      "id": "UzMoTFKKLjOP",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_news = Counter()\n",
        "bigrams_news = Counter()\n",
        "trigrams_news = Counter()\n",
        "\n",
        "for sentence in sentences_news:\n",
        "    unigrams_news.update(sentence)\n",
        "    bigrams_news.update(ngrammer(sentence, n=2))\n",
        "    trigrams_news.update(ngrammer(sentence, n=3))"
      ],
      "metadata": {
        "id": "-uyb1aS_MARC"
      },
      "id": "-uyb1aS_MARC",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import lil_matrix, csr_matrix, csc_matrix"
      ],
      "metadata": {
        "id": "Sj80YBAmMpxr"
      },
      "id": "Sj80YBAmMpxr",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# делаем то же, что на семинаре, но теперь для биграмм и униграмм\n",
        "matrix_news = lil_matrix((len(bigrams_news),\n",
        "                        len(unigrams_news)))\n",
        "\n",
        "id2word_news = list(unigrams_news)\n",
        "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
        "\n",
        "id2bigram_news = list(bigrams_news)\n",
        "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigram_news)}\n",
        "\n",
        "for ngram in trigrams_news:\n",
        "    bigram, word = ngram.rsplit(\" \", 1)\n",
        "    matrix_news[bigram2id_news[bigram], word2id_news[word]] =  (trigrams_news[ngram]/\n",
        "                                                                     bigrams_news[bigram])\n",
        "\n",
        "matrix_news = csc_matrix(matrix_news)"
      ],
      "metadata": {
        "id": "vAuLUkSOFrBQ"
      },
      "id": "vAuLUkSOFrBQ",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(matrix, id2word, word2id, id2bigram, bigram2id, n=100, start=\"<start> <start>\"):\n",
        "    text = []\n",
        "    current_idx = bigram2id[start]\n",
        "    prev_word=[\"<start>\"]\n",
        "\n",
        "    for i in range(n):\n",
        "\n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
        "        text.append(id2word[chosen])\n",
        "\n",
        "        if id2word[chosen] == '<end>':\n",
        "            prev_word=[\"<start>\"]\n",
        "            chosen = word2id['<start>']\n",
        "\n",
        "        prev_word.append(id2word[chosen])\n",
        "        current_idx = bigram2id[\" \".join(prev_word)]\n",
        "        prev_word = prev_word[1:]\n",
        "\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "0--Yvu7-KwoL"
      },
      "id": "0--Yvu7-KwoL",
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(generate(matrix_news, id2word_news, word2id_news, id2bigram_news, bigram2id_news))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMMwf8tMLOXk",
        "outputId": "d5f898e9-0266-4ef9-f3c5-34debb689e2f"
      },
      "id": "OMMwf8tMLOXk",
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "глава комитета <end> дом был оцеплен частями милиции на место оперативники задержали молодого человека который указал им место где он со своими зарубежными коллегами более опытным в противостоянии этому злу прежде всего неалбанской его части <end> входе перестрелки с правительственными войсками и сепаратистами из организации тигры освобождения тамил илама тоти происходят практически повсеместно на северо-востоке от голчука и всего в andava и forus <end> постановление принято в связи с террористическими актами в столице значительно увеличилось количество желающих безвозмездно сдать свою кровь для нужд пострадавших в результате 6 человек получили ножевые ранения в прибрежном городе голчук <end> выступая по местному времени\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Перплексия"
      ],
      "metadata": {
        "id": "XQvyLghFWCve"
      },
      "id": "XQvyLghFWCve"
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_joint_proba_markov_assumption(text, bigram_counts, trigram_counts):\n",
        "    prob = 0\n",
        "    for ngram in ngrammer(['<start>'] + normalize(text) + ['<end>'], 3):\n",
        "        bigram, word = ngram.rsplit(\" \", 1)\n",
        "        if bigram in bigram_counts and ngram in trigram_counts:\n",
        "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
        "        # small value for unk words\n",
        "        else:\n",
        "            prob += np.log(2e-5)\n",
        "\n",
        "    return np.exp(prob)"
      ],
      "metadata": {
        "id": "VWr_ZLTKXWSb"
      },
      "id": "VWr_ZLTKXWSb",
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_join_proba_markov_assumption(\n",
        "    text, smaller_ngram_counts, ngram_counts,\n",
        "    n=1, tokenized=True\n",
        "):\n",
        "    prob = 0\n",
        "    # if tokenized:\n",
        "    #     tokenized_text = text\n",
        "    # else:\n",
        "    #     tokenized_text = [\"<start>\"] * n + normalize(text) + [\"<end>\"]\n",
        "\n",
        "    for ngram in ngrammer(tokenized_text, n=n+1):\n",
        "        left_ngram, word = ngram.rsplit(\" \", 1)\n",
        "        if left_ngram in smaller_ngram_counts and ngram in ngram_counts:\n",
        "            prob += np.log(\n",
        "                ngram_counts[ngram] / smaller_ngram_counts[left_ngram]\n",
        "            )\n",
        "        else:\n",
        "            prob += np.log(2e-5)\n",
        "    return prob, len(tokenized_text) - n - 1"
      ],
      "metadata": {
        "id": "7EMfEE-rXgsX"
      },
      "id": "7EMfEE-rXgsX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = []\n",
        "\n",
        "for sent in sents_news_val:\n",
        "    perplexity.append(compute_joint_proba_markov_assumption(\" \".join(sent), bigrams_news, trigrams_news))\n",
        "\n",
        "np.mean(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy4BU_1FZmZ4",
        "outputId": "4cb2e278-b3cf-4d82-a4c4-b814cfc85c6b"
      },
      "id": "Hy4BU_1FZmZ4",
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(3.1285607395677934e-25)"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}